For the implementation of the two pass MapReduce pipeline the Hadoop Framework was used. Hadoop is an open-source implementation of a distributed file system (HDFS) with a high level API that allows to easily run the Mapper and Reducer Tasks each in parallel on distributed nodes. All the code for the Mapper and Reducer Tasks as well as preprocessing and postprocessing steps was written in Java. In the scope of this project a pipeline was created which in sequential order consists of the following modules:
\begin{enumerate}
\item preprocessing of the input file to create data chunks 
\item setting of the threshold parameters so that they are accessible by the Mapper and Reducer classes of the Hadoop API
\item running the first pass of MapReduce to create itemset candidates
\item running the second pass of MapReduce to find frequent itemsets
\item postprocessing of the output of the second MapReduce pass
\end{enumerate}
A user executes this pipeline by calling the implemented script
\begin{center}
\textit{find\_frequent\_itemsets.sh data.dat k s}
\end{center}
where \textit{data.dat} is a transaction file in the format described in section \ref{test data}, \textit{k} is the desired number of data chunks that \textit{data.dat} is split into and \textit{s} is the support threshold in percentage (see Appendix \ref{scripts}).
For each module this section gives insight into how it is processing the given input. Section \ref{preprocessing} describes the splitting of the test data into chunks and the parameter setting so that they are accessible by the Hadoop Modules. Section \ref{first pass details} and \ref{second pass details} give details about the first and second run of MapReduce. These two sections give Hadoop specific implementation details. \ref{postprocessing} describes how the output of the second MapReduce run is postprocessed.

\subsection{Data Preprocessing and Parameter Setting}\label{preprocessing}
As a first step in the pipeline the complete list of transactions is split into $k$ chunks. In order to find out how many transactions of the complete list go into each chunk it is required to know how many transactions $l$ the complete list contains. If $l$ and $k$ are known it is possible to go over the complete list and write $\frac{l}{k}$ transactions into each chunk. To find $l$ we go over the complete list once, then we go over the list again and write $\frac{l}{k}$ transactions into each file. The main loop to get the chunks is listed in Listing 1 (Note that we round up for $\frac{l}{k}$, if the initial file can not be split into $k$ parts equally). The complete code is listed in Appendix \ref{java}.

\begin{lstlisting}[caption={writing the data chunks},captionpos=b,label={lst:create chunks}]
public void create_chunks(String file_path, int k,
	String output_directory) throws IOException{
	
int total_number_of_transactions =
			get_number_transactions(file_path);
float transactions_per_chunk_float =
		 (float) total_number_of_transactions/k;
int transactions_per_chunk =
		 (int) Math.ceil(transactions_per_chunk_float);
	
/*
	initialize all counters, file_writers,..., see Appendix 5.3
*/

while ((transaction = file_reader.readLine())!=null){
	transactions_written_to_chunk++;
	total_line_counter++;
	chunk_writer.println(transaction);
			
	if (transactions_written_to_chunk == 
			transactions_per_chunk){
			
		chunk_writer.close();
		chunk_counter++;
		output_chunk = 
				get_filename_for_next_chunk(chunk_counter);			
		if (total_line_counter!=total_number_of_transactions)
		{
			chunk_writer = get_chunk_writer(output_directory,
							 output_chunk);
		}
		transactions_written_to_chunk = 0;
	}
		
}
chunk_writer.close();
file_reader.close();	
}
\end{lstlisting}

The above code writes the data chunks to a local directory from which they are copied to the Hadoop machine by the subscript
\begin{center}
\textit{copy\_chunks\_to\_hadoop.sh path\_from path\_to}
\end{center} 
where the path names \textit{path\_from} and \textit{path\_to} are generated by the calling parent script
\textit{find\_frequent\_itemsets.sh}. Finally the first pipeline step writes the total threshold $s$ and the number of lines $l_{total}$ of the initial file to the configuration files \textit{config/support\_threshold.txt} and \textit{config/number\_lines.txt} on the Hadoop machine where they are accessable to the following runs of MapReduce (see \ref{first pass details} and \ref{second pass details}). 
\subsection{The First MapReduce Pass}\label{first pass details}
After the data chunks are copied to the hadoop machine and the parameters for the thresholds are set the parent script \textit{find\_frequent\_itemsets.sh} executes the first run of MapReduce. The first run of mappers each process a data chunk and find frequent itemsets using A-Priori. This section does not examine the implemented A-Priori algorithm as it would exceed the scope of this report. Well documented code for the A-Priori function is listed in Appendix \ref{java}. Also the actual code for the mappers and reducers is very similar to the pseudocode given in \ref{first map task}, \ref{second map task} and thus not examined here (see Appendix \ref{java} for well documented code of the mappers and reducers). On the other hand this section covers technical details about the use of the Hadoop framework. The two aspects that appeared challenging to implement in Hadoop were:
\begin{itemize}
\item setting the threshold parameter for the data chunks in the Mapper class of the Hadoop job. Each mapper needs to calculate the lower threshold dynamically according to the total threshold and the total number of transactions. Let $l_{chunk}$ be the number of transactions in a data chunk, $l_{total}$ be the number of transactions in the initial file and $s$ be the total support threshold. The lower threshold $p$ is computed as $\frac{l_{chunk}}{l_{total}}*s$ where we round to the next lower number if we get a value $p\not\in N$. How to obtain $l_{total}$ and $s$ in each mapper is explained in \ref{set parameter}
\item for our application of the Hadoop framework it was required that each mapper processes the complete content of a data chunk. This is not the standard use of Hadoop. In the given \textit{WordCount.java} for example each mapper only processes one line of a file at a time. How to configure Hadoop to process complete data chunks in each mapper is explained in \ref{complete content}.
\end{itemize}

\subsubsection{Getting the threshold parameter}\label{set parameter}
As mentioned in \ref{preprocessing} the support threshold and the total number of transactions were written to Hadoop accessable configuration files. The hadoop class \textit{Mapper} from which the implemented Mapper class inherits contains a \textit{setup} method which was utilized to read these parameters from the configuration files. By default the \textit{setup} method is called at the beginning of a Mapper (Reducer) Task and is capable of setting the parameters as class variables where they are readable by the \textit{map} (\textit{reduce}) method. Assuming that the support threshold was written to the file \textit{config/support\_threshold.txt} and the total number of transactions to \textit{config/number\_transactions.txt} on the Hadoop machine in the format \textit{parameter\_name:p} they can be obtained as listed in Listing \ref{lst:setting parameter}.

\begin{lstlisting}[caption={setting the threshold parameter},captionpos=b,label={lst:setting parameter}]
public static class Mapper extends
			Mapper<Object, BytesWritable, Text, IntWritable> {
			
private int total_support_threshold;
private int total_number_transactions;

public void setup(Context context) throws IOException {
	Path path = context.getWorkingDirectory();
	String path_as_string= path.toString();
	String threshold_file_path_string = 
		path_as_string.concat("/config/support_threshold.txt");
	String transactioncount_file_path_string = 
		path_as_string.concat("/config/number_transactions.txt");	
	Path threshold_file_path = new Path(threshold_file_path_string);
	Path transaction_count_path =
			 new Path(transactioncount_file_path_string);
			 
	FileSystem fs = FileSystem.get(context.getConfiguration());
	FSDataInputStream in = fs.open(threshold_file_path);
	String line;
	String[] temp = new String[2];
	line = in.readLine());
	in.close();
	temp = line.split(":");
	String threshold_string = temp[1];
	
	FSDataInputStream in = fs.open(transaction_count_path);
	line = in.readLine());
	in.close();
	temp = line.split(":");
	String transactioncount_string = temp[1];
	
	this.total_support_threshold = Integer.parseInt(threshold_string);
	this.transaction_count = Integer.parseInt(transactioncount_string);
}
public void map(Object key, BytesWritable value, Context context)
				throws IOException, InterruptedException {
				
	/*
	convert key to Array of Strings;
	number of Strings in Array gives us 
			the number of transactions in data chunk;
	compute the lower threshold for the data chunk, using
			total_support_threshold (set in setup method),
			total_number_transactions (set in setup method) and	
			number of transactions in data chunk;
	run A-Priori with lower threshold;
	*/
				
}

}
\end{lstlisting}

\subsubsection{Passing complete file content to mappers}\label{complete content}
The manner in which Hadoop passes the content of an input path to the individual mapper functions can be configured by creating a class which inherits from the Hadoop class \textit{FileInputFormat} and setting it as the \textit{InputFormatClass} of the Hadoop job. Listing \ref{lst:passing complete} shows the implemented Format Class.

\begin{lstlisting}[caption={InputFormatClass to pass complete data chunk content to mappers},captionpos=b,label={lst:passing complete}]
public class WholeFileInputFormat 
		extends FileInputFormat<NullWritable, BytesWritable>{	
@Override
protected boolean isSplitable(JobContext context, 
							Path filename) {        
return false;
}
@Override
public RecordReader<NullWritable, BytesWritable>
		createRecordReader(InputSplit split, 
					TaskAttemptContext context)
 						 	throws IOException {   	
return new WholeFileRecordReader();       
}   
}
\end{lstlisting}

The two methods that are overwritten from the parent class \textit{FileInputFormat} are \textit{isSplitable} and \textit{createRecordReader}. \textit{isSplitable} just returns false which assures that a data chunk is not split further and handed to separate mappers. \textit{createRecordReader} returns an instance of the class \textit{WholeFileRecordReader} which inherits from \textit{RecordReader}. The class \textit{WholeFileRecordReader} defines in the method \textit{nextKeyValue} how to break the file splits into key-value pairs for input to the mappers. In our case we want the file splits to be passed to the mappers as one. Therefore the method \textit{nextKeyValue} was overwritten to read complete file splits without breaking them further. Appendix \ref{java} lists the implemented class \textit{WholeFileRecordReader}.
\subsection{The Second MapReduce Pass}\label{second pass details}

The first MapReduce pass created a list of candidate itemsets. Each mapper in the second run processes one data chunk and counts how often each candidate occurs in it. Again the Java code for the second run of mappers and reducers is very similar to the pseudocode given in \ref{second map task} and \ref{second reduce task}. Well documented code for the second run of mappers and reducers can be found in Appendix \ref{java}. To not exceed the volume of this report this section rather focusses on the technical detail of how to pass the candidate itemsets to each mapper in the second pass which is explained in \ref{passing candidates}
\subsubsection{Passing the candidate itemsets to mappers}\label{passing candidates}
The output of the first pass of MapReduce is a text file which contains one candidate itemset per line. Each line is in the key-value format as shown below. The value part of these pairs is not of interest and can be discarded. We are only interested in the itemset part of each line. The first few lines of the candidate itemset file, which by default is called \textit{part-r-00000} will look similar to this:
\begin{align*}
&[0]&1
\\ &[100, 362]&1
\\ &[100]	&1
\\ &[104]	&1
\\ &[11, 793]	&1
\end{align*}

In order for the second set of mappers to read the candidate file it gets copied to a temporary directory by the parent script \textit{find\_frequent\_itemsets.sh} before starting the Hadoop job for the second pass. We utilized the inherited \textit{setup} method of the Mapper class to read the candidate file. Listing \ref{lst:reading candidates} lists the main loop to obtain the candidates. The complete code is listed in the Appendix.

\begin{lstlisting}[caption={reading the candidate itemset file},captionpos=b,label={lst:reading candidates}]
public static class SecondPassMapper extends
			Mapper<Object, BytesWritable, Text, IntWritable> {
			
public ArrayList<HashSet<Integer>> candidateItemsets = 
				new ArrayList<HashSet<Integer>>();
				
public void setup(Context context) throws IOException {
/*
	initialize hadoop file reader for candidate itemset file
			(see Apendix)
*/
while ((line = in.readLine()) != null) {
	temp = line.split("\\t");
	candidatesString = temp[0].substring(1, temp[0].length());
	candidatesString = 
		candidatesString.substring(0, candidatesString.length() - 1);
	itemset = candidatesString.split(", ");
	tempSet = new HashSet<Integer>();
	for (String item : itemset) {
		tempSet.add(Integer.parseInt(item));
	}
	candidateItemsets.add(tempSet);
}
in.close();
}
/*
actual code for the map function goes here (see Appendix)
*/
}
\end{lstlisting}


\subsection{Output Postprocessing}\label{postprocessing}
As a result of the second MapReduce pass we get a textfile which contains all frequent itemsets in the format generated by Hadoop. In this format we get an unsorted list of itemset-count pairs. In the last pipeline step we transform this format into the desired format of sorted itemsets by sorting it respective to the itemsets count. Then we simply write the sorted list into a final output file \textit{frequent\_itemsets} where the first line contains the total number of frequent itemsets and the following lines each contain one itemset count pair in descending order regarding the itemsets count.